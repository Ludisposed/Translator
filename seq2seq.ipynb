{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dependencies\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helper\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = helper.vocab_size\n",
    "input_embedding_size = vocab_size * 2\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape = (None, None), dtype = tf.int32, name = 'encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(shape = (None,), dtype = tf.int32, name = 'encoder_inputs_length')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype = tf.int32, name = 'decoder_targets')\n",
    "decoder_targets_length = encoder_inputs_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype = tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.rnn_cell import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = LSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    " (encoder_fw_final_state,\n",
    "  encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "encoder_final_state_c = tf.concat((encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "encoder_final_state_h = tf.concat((encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "encoder_final_state = LSTMStateTuple(c = encoder_final_state_c, h = encoder_final_state_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = LSTMCell(decoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "ecoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_targets_length)\n",
    "    initial_input = eos_step_embedded\n",
    "    initial_cell_state = encoder_final_state\n",
    "    initial_cell_output = None\n",
    "    initial_loop_state = None\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "    \n",
    "    def get_next_input():\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    \n",
    "    elements_finished = (time >= decoder_targets_length)\n",
    "    finished = tf.reduce_all(elements_finished)\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    \n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "    \n",
    "    return (elements_finished,\n",
    "           input,\n",
    "           state,\n",
    "           output,\n",
    "           loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_outputs_ta, decoder_final_state,_ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1,decoder_dim))\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "learning_rate = 5e-3\n",
    "#loss function\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "#train it \n",
    "train_op = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By the w\n",
      "ay, toge\n",
      "ther wit\n",
      "h this p\n",
      "ost I am\n",
      " also re\n",
      "leasing \n",
      "code on \n",
      "Github t\n",
      "hat allo\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "batches = helper.generate_sequence(8, batch_size)\n",
    "for b in next(batches)[:10]:\n",
    "    print(helper.decode(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    input_batch = batch[:-1]\n",
    "    output_batch = batch[1:]\n",
    "    encoder_inputs_, encoder_input_lengths_ = helper.batch(input_batch)\n",
    "    decoder_targets_,_ = helper.batch(output_batch)\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 3.8551406860351562\n",
      "  sample 1:\n",
      "    input          > [37  4 30 22 38 17 30 39]\n",
      "    decode input   > By the w\n",
      "    output         > [27  4 19 30 22 34 10 17]\n",
      "    decode output  > ay, toge\n",
      "    predicted      > [26 23 22 20  5 14 38 12]\n",
      "    decode predict > RNt;rlhI\n",
      "  sample 2:\n",
      "    input          > [27  4 19 30 22 34 10 17]\n",
      "    decode input   > ay, toge\n",
      "    output         > [22 38 17  5 30 39 11 22]\n",
      "    decode output  > ther wit\n",
      "    predicted      > [ 4  4  2  2 16 16 16 23]\n",
      "    decode predict > yySS’’’N\n",
      "  sample 3:\n",
      "    input          > [22 38 17  5 30 39 11 22]\n",
      "    decode input   > ther wit\n",
      "    output         > [38 30 22 38 11 40 30 31]\n",
      "    decode output  > h this p\n",
      "    predicted      > [ 8 23 23  6 20  6 26 20]\n",
      "    decode predict > GNNf;fR;\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 2.3935910576255992e-05\n",
      "  sample 1:\n",
      "    input          > [37  4 30 22 38 17 30 39]\n",
      "    decode input   > By the w\n",
      "    output         > [27  4 19 30 22 34 10 17]\n",
      "    decode output  > ay, toge\n",
      "    predicted      > [27  4 19 30 22 34 10 17]\n",
      "    decode predict > ay, toge\n",
      "  sample 2:\n",
      "    input          > [27  4 19 30 22 34 10 17]\n",
      "    decode input   > ay, toge\n",
      "    output         > [22 38 17  5 30 39 11 22]\n",
      "    decode output  > ther wit\n",
      "    predicted      > [22 38 17  5 30 39 11 22]\n",
      "    decode predict > ther wit\n",
      "  sample 3:\n",
      "    input          > [22 38 17  5 30 39 11 22]\n",
      "    decode input   > ther wit\n",
      "    output         > [38 30 22 38 11 40 30 31]\n",
      "    decode output  > h this p\n",
      "    predicted      > [38 30 22 38 11 40 30 31]\n",
      "    decode predict > h this p\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 6.742837285855785e-06\n",
      "  sample 1:\n",
      "    input          > [37  4 30 22 38 17 30 39]\n",
      "    decode input   > By the w\n",
      "    output         > [27  4 19 30 22 34 10 17]\n",
      "    decode output  > ay, toge\n",
      "    predicted      > [27  4 19 30 22 34 10 17]\n",
      "    decode predict > ay, toge\n",
      "  sample 2:\n",
      "    input          > [27  4 19 30 22 34 10 17]\n",
      "    decode input   > ay, toge\n",
      "    output         > [22 38 17  5 30 39 11 22]\n",
      "    decode output  > ther wit\n",
      "    predicted      > [22 38 17  5 30 39 11 22]\n",
      "    decode predict > ther wit\n",
      "  sample 3:\n",
      "    input          > [22 38 17  5 30 39 11 22]\n",
      "    decode input   > ther wit\n",
      "    output         > [38 30 22 38 11 40 30 31]\n",
      "    decode output  > h this p\n",
      "    predicted      > [38 30 22 38 11 40 30 31]\n",
      "    decode predict > h this p\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 3.941079739888664e-06\n",
      "  sample 1:\n",
      "    input          > [37  4 30 22 38 17 30 39]\n",
      "    decode input   > By the w\n",
      "    output         > [27  4 19 30 22 34 10 17]\n",
      "    decode output  > ay, toge\n",
      "    predicted      > [27  4 19 30 22 34 10 17]\n",
      "    decode predict > ay, toge\n",
      "  sample 2:\n",
      "    input          > [27  4 19 30 22 34 10 17]\n",
      "    decode input   > ay, toge\n",
      "    output         > [22 38 17  5 30 39 11 22]\n",
      "    decode output  > ther wit\n",
      "    predicted      > [22 38 17  5 30 39 11 22]\n",
      "    decode predict > ther wit\n",
      "  sample 3:\n",
      "    input          > [22 38 17  5 30 39 11 22]\n",
      "    decode input   > ther wit\n",
      "    output         > [38 30 22 38 11 40 30 31]\n",
      "    decode output  > h this p\n",
      "    predicted      > [38 30 22 38 11 40 30 31]\n",
      "    decode predict > h this p\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, out, pred) in enumerate(zip(fd[encoder_inputs].T, fd[decoder_targets].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input          > {}'.format(inp))\n",
    "                print('    decode input   > {}'.format(helper.decode(inp)))\n",
    "                print('    output         > {}'.format(out))\n",
    "                print('    decode output  > {}'.format(helper.decode(out)))\n",
    "                print('    predicted      > {}'.format(pred))\n",
    "                print('    decode predict > {}'.format(helper.decode(pred)))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0000 after 3001000 example (batch_size=1000)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAk9sT+MbBcUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1113de198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} example (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyBy the way, together with this post I am also releasing code on Github that allows you to train \n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "\n",
    "init_input = [[helper.char_to_ix[ch] for ch in helper.data[:8]]]\n",
    "strings = helper.decode(init_input[0])\n",
    "def next_feed(previous_output):\n",
    "    encoder_inputs_, encoder_input_lengths_ = helper.batch(previous_output)\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "    }\n",
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    previous_output = init_input\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed(previous_output)\n",
    "        predict_ = sess.run(decoder_prediction, fd)\n",
    "        for pred in predict_.T:\n",
    "            strings += helper.decode(pred)\n",
    "        previous_output = predict_.T\n",
    "    print(strings)\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
